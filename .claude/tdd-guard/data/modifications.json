{
  "session_id": "5564189d-de99-4a18-9b66-5d8c489d83a6",
  "transcript_path": "/Users/hue/.claude/projects/-Users-hue-code-Dopemux-ChatRipperXXX/5564189d-de99-4a18-9b66-5d8c489d83a6.jsonl",
  "hook_event_name": "PreToolUse",
  "tool_name": "Write",
  "tool_input": {
    "file_path": "/Users/hue/code/Dopemux-ChatRipperXXX/zen_precommit.changeset",
    "content": "diff --git a/.claude/hooks/pre_context_budget.py b/.claude/hooks/pre_context_budget.py\nindex 8a6bf42..b281821 100755\n--- a/.claude/hooks/pre_context_budget.py\n+++ b/.claude/hooks/pre_context_budget.py\n@@ -4,9 +4,9 @@ import json\n import os\n CONPORT_STRICT = os.getenv(\"HOOKS_CONPORT_STRICT\", \"1\") == \"1\"\n EXA_MIN_QUERY_LEN = int(os.getenv(\"HOOKS_EXA_MIN_QUERY_LEN\", \"3\"))\n-CLAUDE_CONTEXT_MAX_RESULTS = int(os.getenv(\"HOOKS_CLAUDE_CONTEXT_MAX_RESULTS\", \"5\"))\n-TASKMASTER_DEFAULT_LIMIT = int(os.getenv(\"HOOKS_TASKMASTER_LIMIT\", \"5\"))\n-ZEN_MAX_FILES = int(os.getenv(\"HOOKS_ZEN_MAX_FILES\", \"2\"))\n+CLAUDE_CONTEXT_MAX_RESULTS = int(os.getenv(\"HOOKS_CLAUDE_CONTEXT_MAX_RESULTS\", \"3\"))\n+TASKMASTER_DEFAULT_LIMIT = int(os.getenv(\"HOOKS_TASKMASTER_LIMIT\", \"3\"))\n+ZEN_MAX_FILES = int(os.getenv(\"HOOKS_ZEN_MAX_FILES\", \"1\"))\n GENERIC_TERMS = {\"help\",\"docs\",\"documentation\",\"fix\",\"error\",\"issue\",\"bug\",\"search\"}\n def out(decision, reason): print(json.dumps({\"decision\": decision, \"permissionDecisionReason\": reason}), flush=True)\n def main():\ndiff --git a/CLAUDE.md b/CLAUDE.md\nindex 009aee9..855f9e0 100644\n--- a/CLAUDE.md\n+++ b/CLAUDE.md\n@@ -67,12 +67,14 @@ Produce **small, correct patches** that build and pass first run, leveraging **M\n * **PreToolUse**\n   * Block/ask on dangerous shell (`sudo`, `rm`) and network installs (`curl`, `wget`, `pip install`, `npm install`).\n   * Prevent reading sensitive files (`.env`, `secrets/**`).\n-  * Token thrift nudges:\n-    * **ConPort** → use summaries/search with small `limit` before full context.\n-    * **Claude-Context** → cap results ≤ 3 (down from 5).\n-    * **TaskMaster** → use `status` filters + `withSubtasks=false` (saves ~5k tokens).\n-    * **Zen** → limit context files ≤2, reuse `continuation_id` for long chats.\n-    * **Exa** → refine over-broad queries, min 5 chars (up from 3).\n+  * **Token thrift nudges** (Target: <25k MCP context):\n+    * **Zen (~29k tokens)** → MANDATORY `files≤1` + `continuation_id` reuse; avoid for simple tasks\n+    * **TaskMaster (~21k tokens)** → ALWAYS use `status=pending` + `withSubtasks=false` (saves ~15k)\n+    * **ConPort (~17k tokens)** → use `limit=3-5` on searches; summaries before full context\n+    * **Serena (~15k tokens)** → prefer symbolic tools over full file reads\n+    * **Claude-Context** → cap results ≤3 (down from 5)\n+    * **Exa** → refine over-broad queries, min 5 chars; prefer specific lib/error terms\n+    * **Fast-Markdown (~4k tokens)** → use section-specific queries vs full file reads\n * **PostToolUse** (on write/edit, including **Serena** changes): run `ruff`, `mypy`, and `pytest --cov-fail-under=${HOOKS_COV_MIN:-90}`.\n \n See `.claude/hooks/README.md` for matcher examples (include both `mcp__claude-context__.*` and `mcp__claude_context__.*`).\n@@ -91,6 +93,53 @@ See `.claude/hooks/README.md` for matcher examples (include both `mcp__claude-co\n \n **Enforcement**: This is a hard requirement - using basic bash commands when better tools exist is a workflow violation.\n \n+## MCP Server Token Budget Management (CRITICAL)\n+\n+**Current Usage: ~93k tokens from MCP tools alone**\n+\n+### High-Impact Token Reduction (Priority Order)\n+\n+1. **Zen Server (~29k tokens)** - MOST EXPENSIVE\n+   - Use ONLY for complex multi-step workflows requiring expert analysis\n+   - MANDATORY: `files≤1` parameter (not ≤2)\n+   - ALWAYS reuse `continuation_id` for follow-ups\n+   - Consider basic tools first before Zen workflows\n+\n+2. **TaskMaster (~21k tokens)** - SECOND MOST EXPENSIVE  \n+   - DEFAULT pattern: `status=pending withSubtasks=false` (saves ~15k tokens)\n+   - Use `get_task id=X` for specific tasks vs `get_tasks`\n+   - Filter by status before expanding with subtasks\n+   - Batch operations when possible\n+\n+3. **ConPort (~17k tokens)** - THIRD MOST EXPENSIVE\n+   - ALWAYS use `limit=3-5` on searches and gets\n+   - Use `search_*_fts` with targeted queries vs broad gets\n+   - Summary-first approach: search → targeted get → full context (last resort)\n+\n+4. **Serena (~15k tokens)** - FOURTH MOST EXPENSIVE\n+   - Prefer `find_symbol` + `get_symbols_overview` over `read_file`\n+   - Use `search_for_pattern` with specific regex vs broad file reads\n+   - Target specific functions/classes, not entire files\n+\n+### Token-Efficient Alternatives\n+\n+| Instead of | Use This | Token Savings |\n+|------------|----------|---------------|\n+| `zen` workflow | Basic tools (Read, Edit, Bash) | ~25k tokens |\n+| `get_tasks` unlimited | `get_tasks status=pending withSubtasks=false` | ~15k tokens |\n+| ConPort full context | `search_decisions_fts` with `limit=3` | ~10k tokens |\n+| `read_file` entire file | `find_symbol` + targeted reads | ~5k tokens |\n+| `get_active_context` | `search_custom_data_value_fts` | ~3k tokens |\n+\n+### Emergency Token Budget Protocol\n+\n+If context exceeds 100k tokens:\n+1. **STOP** using Zen tools immediately  \n+2. Switch TaskMaster to `status=pending withSubtasks=false`\n+3. Use ConPort with `limit=1-2` maximum\n+4. Prefer Serena symbolic tools over file reads\n+5. Consider `/clear` to reset context\n+\n ## Core Project Constraints\n \n * **Privacy**: Policy Shield ≥99.5% coverage (≥99.9% strict), attachments NEVER cloud, local-first processing with optional cloud via `--allow-cloud`"
  }
}